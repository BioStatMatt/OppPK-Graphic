\documentclass{article}
\usepackage{amsmath}
\begin{document}
\section{Two-compartment model}
The two-compartment pharmacokinetic model is expressed as a system of two ordinary differential equations as follows, where $m_1$ and $m_2$ are the masses of drug in the central and peripheral compartments, respectively.

\begin{align}
\frac{dm_1}{dt} &= -k_{10}m_1 - k_{12}m_1 + k_{21}m_2 + k_R \nonumber \\
\frac{dm_2}{dt} &= \phantom{-k_{10}m_1} + k_{12}m_1 - k_{21}m_2 \nonumber
\end{align}

The concentration of drug in the central compartment is given by $c_1 = m_1/v_1$, where $v_1$ is the volume of the central compartment. The parameters $k_{10}$, $k_{12}$, $k_{21}$, and $k_R$ are described in Table \ref{tab:pkpars}

\begin{table}
\begin{tabular}{lll} \hline
Parameter & Units & Description \\ \hline
$k_{10}$ & h$^{-1}$ & Elimination rate from central compartment\\
$k_{12}$ & h$^{-1}$ & Distribution rate from central to peripheral compartment\\
$k_{21}$ & h$^{-1}$ & Distribution rate from peripheral to central compartment\\
$k_R$  & g$\cdot$h$^{-1}$ & Infusion rate into central compartment\\
$v_1$  & L & Volume of central compartment\\
\hline
\end{tabular}
\caption{Two-compartment model parameter units (SI) and descriptions. \label{tab:pkpars}}
\end{table}

\section{Bayes prediction model}
Concentration measurements are generally treated using a nonlinear regression with either additive or multiplicative error, where the former is expressed as follows:
\begin{displaymath}
c_{ij} = \eta_i(t_{ij}, \theta_i) + \epsilon_{ij}
\end{displaymath}
\noindent In this expression, $c_{ij}$ is the measured concentration for subject $i = 1 \ldots n$ at time $t_{ij}$ for $j = 1 \ldots m_i$, $\eta_i(t_{ij}, \theta_i)$ is the two-compartment model solution for subject $i$ at time $t_{ij}$ given parameters $\theta_i = [k_{10i}, k_{12i}, k_{21i}, v_{1i}]$, and $\epsilon_{ij}$ represents i.i.d. random error with mean zero. The multiplicative error model is expressed by substituting $c_{ij}$ and $\eta_i(t_{ij}, \theta_i)$ with their natural logarithm, respectively. By specifying the random error density function, say $f(\epsilon_{ij}, \sigma)$, the subject-specific likelihood function is 
\begin{displaymath}
L_i(\theta_i, \sigma) = \prod_{j=1}^{m_i} f(c_{ij} - \eta_i(t_{ij}, \theta_i), \sigma).
\end{displaymath}
\noindent Thus, given a prior distribution $\pi_0(\theta_i)$, the subject-specific posterior is proportional to $\pi_i(\theta_i, \sigma) \propto \pi_0(\theta_i,\sigma)L_i(\theta_i,\sigma)$.

In the current context, $f$ is taken as the normal density function with mean zero and standard deviation $\sigma$, and the prior distribution is generated to satisfy the following:
\begin{align}
[\log k_{10}, \log k_{12}, \log k_{21}, \log v_1] &\sim N_4(\mu_0, \Sigma_0) \\
[\sigma] &\sim \Gamma(\alpha_0, \beta_0)
\end{align}
\noindent where $N_4(\mu_0, \Sigma_0)$ represents the 4-variate normal distribution with mean $\mu_0$ and covariance matrix $\Sigma_0$, and $\Gamma(\alpha_0, \beta_0)$ represents the gamma distribution with shape $\alpha_0$ and rate $\beta_0$. 

Due to the nonlinearity of the two-compartment model, the posterior distribution does not take a familiar form, and posterior summaries must be approximated. In particular, we sought to compute 95% credible bands for subject-specific concentration-time curves, and for summaries of the curve (e.g., the area under the curve, or the length of time in which the concentration exceeds a specified value). Monte Carlo techniques are often used to approximate these quantities. However, because these posterior summaries are presented in a web application in real time, we sought alternatives that were less computationally intensive and deterministic. We consider several methods as described in the following sections.

\subsection{Laplace approximation}
We first considered a method that makes use of two approximations. The first is a Laplace approximation to the posterior density, and the second is a first order Taylor approximation of the target summary (i.e., the `delta method'). The Laplace approximation is given as follows
\begin{displaymath}
\pi_i(\theta_i, \sigma) \approx N([\hat{\theta_i},\hat{\sigma}], [-H_{\hat{\theta_i}}]^{-1})
\end{displaymath} 
\noindent where $[\hat{\theta_i},\hat{\sigma}]$ is the posterior mode and $H_{\hat{\theta_i}}$ is the posterior Hessian evaluated at $\theta_i$. The second approximation makes use of the delta method, such that a posterior functional $h(\theta_i)$ has an approximate normal distribution. The first-order Taylor expansion of $h(\theta_i)$ is 
\begin{displaymath}
h(\theta_i) \approx  h(\hat{\theta_i}) + G_{\hat{\theta_i}}(\theta_i - \hat{\theta_i}),
\end{displaymath}
\noindent where $G_{\hat{\theta_i}}$ is the gradient of $h(\theta_i)$ evaluated at $\hat{\theta_i}$. Thus, given that $\theta_i$ has an approximate normal distribution, the same is true for $h(\theta_i)$:
\begin{displaymath}
h(\theta_i) \sim N(h(\hat{\theta_i}), G^T_{\hat{\theta_i}}[-H_{\hat{\theta_i}}]^{-1}G_{\hat{\theta_i}}). 
\end{displaymath}
\noindent An approximate $(1-\alpha)\cdot 100$\% credible interval for $h(\theta_i)$ is thus computed in the usual manner. This method is computationally elegant, since the posterior Hessian and posterior mode can be computed simultaneously by most optimization software routines (e.g., the R function `optim'). Indeed, this method usually has the smallest computational burden (i.e., number of likelihood evaluations) among the three methods considered here.

\subsection{Variational approximation}
\subsection{Quadrature approximation}
\end{document}
