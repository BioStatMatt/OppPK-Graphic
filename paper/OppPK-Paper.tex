\documentclass{article}
\usepackage{amsmath}
\begin{document}
\section{Two-compartment model}
The two-compartment pharmacokinetic model is expressed as a system of two ordinary differential equations as follows, where $m_1$ and $m_2$ are the masses of drug in the central and peripheral compartments, respectively.

\begin{align}
\frac{dm_1}{dt} &= -k_{10}m_1 - k_{12}m_1 + k_{21}m_2 + k_R \nonumber \\
\frac{dm_2}{dt} &= \phantom{-k_{10}m_1} + k_{12}m_1 - k_{21}m_2 \nonumber
\end{align}

The concentration of drug in the central compartment is given by $c_1 = m_1/v_1$, where $v_1$ is the volume of the central compartment. The parameters $k_{10}$, $k_{12}$, $k_{21}$, and $k_R$ are described in Table \ref{tab:pkpars}

\begin{table}
\begin{tabular}{lll} \hline
Parameter & Units & Description \\ \hline
$k_{10}$ & h$^{-1}$ & Elimination rate from central compartment\\
$k_{12}$ & h$^{-1}$ & Distribution rate from central to peripheral compartment\\
$k_{21}$ & h$^{-1}$ & Distribution rate from peripheral to central compartment\\
$k_R$  & g$\cdot$h$^{-1}$ & Infusion rate into central compartment\\
$v_1$  & l & Volume of central compartment\\
\hline
\end{tabular}
\caption{Two-compartment model parameters. \label{tab:pkpars}}
\end{table}

\section{Bayes prediction model}
Concentration measurements are generally treated using a nonlinear regression with either additive or multiplicative error, where the former is expressed as follows:
\begin{displaymath}
c_{ij} = \eta_i(t_{ij}, \theta_i) + \epsilon_{ij}
\end{displaymath}
\noindent In this expression, $c_{ij}$ is the measured concentration for subject $i = 1 \ldots n$ at time $t_{ij}$ for $j = 1 \ldots m_i$, $\eta_i(t_{ij}, \theta_i)$ is the two-compartment model solution for subject $i$ at time $t_{ij}$ given parameters $\theta_i = [k_{10i}, k_{12i}, k_{21i}, v_{1i}]$, and $\epsilon_{ij}$ represents i.i.d. random error with mean zero. The multiplicative error model is expressed by substituting $c_{ij}$ and $\eta_i(t_{ij}, \theta_i)$ with their natural logarithm, respectively. By specifying the random error density function, say $f(\epsilon_{ij}, \sigma)$, the subject-specific likelihood function is 
\begin{displaymath}
L_i(\theta_i, \sigma) = \prod_{j=1}^{m_i} f(c_{ij} - \eta_i(t_{ij}, \theta_i), \sigma).
\end{displaymath}
\noindent Thus, given a prior distribution $\pi(\theta_i)$, the subject-specific posterior is proportional to $\pi(\theta_i,\sigma)L_i(\theta_i,\sigma)$.

In the current context, $f$ is taken as the normal density function with mean zero and standard deviation $\sigma$, and the prior distribution is generated to satisfy the following:
\begin{align}
[\log k_{10}, \log k_{12}, \log k_{21}, \log v_1] &\sim N_4(\mu_0, \Sigma_0) \\
[\sigma] &\sim \Gamma(\alpha_0, \beta_0)
\end{align}
\noindent where $N_4(\mu_0, \Sigma_0)$ represents the 4-variate normal distribution with mean $\mu_0$ and covariance matrix $\Sigma_0$, and $\Gamma(\alpha_0, \beta_0)$ represents the gamma distribution with shape $\alpha_0$ and rate $\beta_0$. 
\end{document}
